# -*- coding: utf-8 -*-
#%%
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QNIOeNMq9e15Vh9yxw1IehoP0B-CUUi5
"""

pip install emoji

import gzip
import csv
import numpy as np
import emoji
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import math

"""#Data Loader and Embedding
(Data pre-processing with GloVe)
"""

#%%
def read_glove_vecs(path):
    with open(path, 'r', encoding = "utf8") as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype = np.float64)
        i = 1
        word_to_idx = {}
        idx_to_words = {}
        for w in sorted(words):
            word_to_idx[w] = i
            idx_to_words[i] = w
            i = i+1
    return word_to_idx, idx_to_words, word_to_vec_map

def sentences_to_indices(x, w_2_i, maxlen):
    m = x.shape[0]
    x_indices = np.zeros((m, maxlen))
    for i in range(m):
        words = (x[i].lower()).split()
        j = 0
        for w in words:
            x_indices[i,j] = w_2_i[w]
            j += 1
    return x_indices

def embed(input, index_to_word, word_to_vec_map):
    input_num, input_len = input.shape
    emb_d = word_to_vec_map['!'].size
    embeddings = np.zeros((input_num, input_len, emb_d))   # create embedding vector
    for i in range(0, input_num):
        for j in range(0, input_len):
            if(input[i][j]):
                embeddings[i][j][:] = word_to_vec_map[index_to_word[input[i][j]]]
    return embeddings

## DATA LOADER ##
class Dataloader():
    """
    dataloader imports data and return parsed data.
    parsed data has sentences => embedded with GloVe, and labels => which is integer.
    """
    def __init__(self, path, glove_size, is_train=True, shuffle=True, batch_size=8):
        basepath = Path(path)
        datapath = Path(basepath/'train_emoji.csv') if is_train else Path(basepath/'test_emoji.csv')
        glovepath = Path(basepath/'glove.6B.50d.txt') if glove_size == 50 else Path(basepath/'glove.6B.100d.txt')
        self.w_2_i, self.i_2_w, self.word_to_vec_map = read_glove_vecs(glovepath)
        self.batch_size = batch_size
        self.sentences, self.labels = self.loadData(datapath)
        self.index = 0
        self.idx = np.arange(0, self.sentences.shape[0])
        if shuffle: np.random.shuffle(self.idx) # shuffle images
        self.maxlen = 0

    def __len__(self):
        n_sentences, _, _= self.sentences.shape
        n_sentences = math.ceil(n_sentences / self.batch_size)
        return n_sentences

    def __iter__(self):
        return datasetIterator(self)

    def __getitem__(self, index):
        x = self.sentences[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        y = self.labels[self.idx[index * self.batch_size:(index + 1) * self.batch_size]]
        
        return x, y

    ## DataLoad
    def loadData(self, path):
        emoji = pd.read_csv(path, names = ['sentence', 'label'], header=None, usecols = [1,0])
        # load sentence data
        x = np.array(emoji.sentence)
        maxlen = len(max(x, key = len).split())
        self.maxlen = maxlen
        x_indices = sentences_to_indices(x, self.w_2_i, maxlen)
        x_embedded = embed(x_indices, self.i_2_w, self.word_to_vec_map)
        
        # load label in one-hot encoding
        y = np.array(emoji.label)
        rows = len(y)
        cols = y.max() + 1
        one_hot = np.zeros((rows, cols)).astype(np.uint8)
        one_hot[np.arange(rows), y] = 1
        one_hot = one_hot.astype(np.float64)

        return x_embedded, y

# for enumerate magic python function returns Iterator
class datasetIterator():
    def __init__(self, dataloader):
        self.index = 0
        self.dataloader = dataloader

    def __next__(self):
        if self.index < len(self.dataloader):
            item = self.dataloader[self.index]
            self.index += 1
            return item
        # end of iteration
        raise StopIteration

trainLoad = Dataloader(path = '/content/drive/MyDrive', glove_size = 50, batch_size=4, is_train = True)
testLoad = Dataloader(path = '/content/drive/MyDrive',  glove_size = 50, batch_size= 1, is_train = False, shuffle=False)

print(trainLoad.sentences.shape, trainLoad.labels.shape)
print(testLoad.sentences.shape, testLoad.labels.shape)

trainLoad_100 = Dataloader(path = '/content/drive/MyDrive', glove_size = 100, batch_size=4,is_train = True)
testLoad_100 = Dataloader(path = '/content/drive/MyDrive',  glove_size = 100, batch_size= 1, is_train = False, shuffle=False)

print(trainLoad_100.sentences.shape, trainLoad_100.labels.shape)
print(testLoad_100.sentences.shape, testLoad_100.labels.shape)


#%%
"""# Model Design"""

class RecurrentNetwork:
    def __init__(self, n_cells, feature_size, optimize_mode, cell_type, learning_rate=0.1, dropout = 0):

        # parameters
        self.n_cells = n_cells     # 셀 개수
        self.lr = learning_rate    # 학습률
        self.optimizer = optimize_mode  # 0:SGD, 1:ADAM
        self.cell_type = cell_type  # 0:RNN, 1:LSTM
        self.feature_size = feature_size # 50 or 100
        self.label_size = 5        # emotion class
        self.feature_size = feature_size # 50 or 100
        self.dp = dropout
        self.s_len = 10

        # weights used 
        self.wxh1 = None
        self.whh1 = None
        self.why1 = None
        self.wxh2 = None
        self.whh2 = None
        self.why2 = None
        self.w3 = None

        # bias used
        self.b1=None
        self.b2=None
        self.b3=None

        # loss container
        self.loss_log = []       # 훈련 손실
        self.test_loss_log = []    # 학습 손실
        self.acc_log = []
        self.test_acc_log = []
    
    #=============UTILS=====================#
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

    def sigmoid(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))
        return a

    def dropout(self, layer, prob):
        if prob != 0:
            temp = np.random.rand(*layer.shape) > prob
            return layer * temp
        else:
            return layer
    
    def initialize_rnn(self):
        self.wxh1 = np.random.normal(0,pow((self.n_cells+self.feature_size)/2, -0.5),size = (self.n_cells, self.feature_size))
        self.whh1 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.why1 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.wxh2 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.whh2 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.why2 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.w3 = np.random.normal(0,pow((self.n_cells+self.label_size)/2, -0.5),(self.label_size,self.n_cells))

        self.b1=0
        self.b2=0
        self.b3=0

    def initialize_lstm(self):

        self.wxh1 = np.random.normal(0,pow((self.n_cells+self.feature_size)/2, -0.5),(4,self.n_cells,self.feature_size)) # f/i/o/g
        self.whh1 = np.random.normal(0,pow(self.n_cells, -0.5),(4,self.n_cells,self.n_cells))
        self.why1 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.wxh2 = np.random.normal(0,pow(self.n_cells, -0.5),(4,self.n_cells,self.n_cells))
        self.whh2 = np.random.normal(0,pow(self.n_cells, -0.5),(4,self.n_cells,self.n_cells))
        self.why2 = np.random.normal(0,pow(self.n_cells, -0.5),(self.n_cells,self.n_cells))
        self.w3 = np.random.normal(0,pow((self.n_cells+self.label_size)/2, -0.5),(5,self.n_cells))

        self.b1 = np.zeros(4)
        self.b2 = np.zeros(4)
        self.b3 = 0.0
    #==================== For-Prop & Back-prop RNN ===================#
    # forpass and backward of RNN
    def forpass_rnn(self, x ,wxh, whh, why, b, output_num):
        hidden = [np.zeros((self.n_cells, 1))] # initialize hidden.
        output = []
        for xt in x: # x = (10, 50) 
            xtt = xt.reshape(xt.shape[0],1)    # 0 dim to 1 dim (50,) -> (50,1)
            z1 = np.dot(whh, hidden[-1]) + np.dot(wxh,xtt) + self.b1
            hidden.append(np.tanh(z1))
            output.append(np.dot(why, hidden[-1]))
        return np.array(hidden), np.array(output[-output_num:]).reshape(output_num,self.n_cells)
    
    def forpass_rnn_full(self, x, y):
        # embedding layer -  x is already embedded in dataloader
        h1_out, z1_out = self.forpass_rnn(x, self.wxh1, self.whh1, self.why1, self.b1, self.s_len)
        h2_out, z2_out = self.forpass_rnn(z1_out, self.wxh2,self.whh2,self.why2,self.b2,1)
        fc_out = self.softmax(np.dot(self.w3, z2_out.T)+self.b3)
        # cross entropy loss
        loss = -np.log(fc_out[y])
        return [x, h1_out, z1_out, h2_out, z2_out, fc_out, loss, y]

    # backpropagation
    def backprop_rnn(self, x ,wxh, whh, why, b, layer_dev, hidden):
        wxh_grad,whh_grad,why_grad,b_grad,hidden_dev,in_grad= [],[],[],[],[],[]
        num = layer_dev.shape[0]
        for i in range(len(x)):  # input의 뒤에서부터 호출하면서 forpass 역순으로 계산
            if num >= i+1:
                why_grad.append(np.dot(layer_dev[-i-1].reshape(self.n_cells,1), hidden[-i-1].T))
                hidden_dev.append(np.dot(why.T,layer_dev[-i-1].reshape(self.n_cells,1)))
            b_grad.append(hidden_dev[i]*(1 - pow(hidden[-i-1],2)))
            whh_grad.append(np.dot(b_grad[i], hidden[-i-2].T))
            
            if num <= i+1:
                hidden_dev.append(np.dot(whh.T,b_grad[i]))
            wxh_grad.append(np.dot(b_grad[i],x[-i-1].reshape((x.shape)[1],1).T))
            in_grad.append(np.dot(wxh.T,b_grad[i]))
        return sum(wxh_grad), sum(whh_grad), sum(why_grad), np.average(np.sum(sum(b_grad),axis=1)), np.array(in_grad)[::-1]
   
    def backprop_rnn_full(self, forpass_log):
        x, h1_out, z1_out, h2_out, z2_out, fc_out, loss, y = forpass_log
        # fully connected layer backprop(softmax dev)
        dev3 = fc_out
        dev3[y] -= 1
        w3_grad= np.dot(dev3.reshape(self.label_size, 1), z2_out.reshape(self.n_cells,1).T)  # linear layer deviation(for weight)
        dev2 = np.dot(self.w3.T, dev3)
        b3_grad= np.mean(dev3)

        # hidden reccurent layer backprop
        wxh2_grad,whh2_grad,why2_grad,b2_grad,dev1 = self.backprop_rnn(z1_out,self.wxh2, self.whh2, self.why2, self.b2, dev2.T, h2_out)
        wxh1_grad,whh1_grad,why1_grad,b1_grad,dev0 = self.backprop_rnn(x,self.wxh1,self.whh1,self.why1,self.b1,dev1,h1_out)

        return wxh1_grad, whh1_grad, why1_grad, wxh2_grad, whh2_grad, why2_grad, w3_grad, b1_grad, b2_grad, b3_grad

    #===================== For-Prop & Back-prop LSTM =====================================#
    def forpass_lstm(self, x ,wxh, whh, why, b, num):
        ct, ht, output, ft, it, ot, gt = [],[],[],[],[],[],[]
        ct.append(np.zeros((self.n_cells,1)))
        ht.append(np.zeros((self.n_cells,1)))
        for xt in x:
            xt = xt.reshape(xt.shape[0],1) # 0 to 1 dimension
            it.append(np.dot(whh[1],ht[-1])+np.dot(wxh[1],xt)+b[1])
            ft.append(np.dot(whh[0],ht[-1])+np.dot(wxh[0],xt)+b[0])
            ot.append(np.dot(whh[2],ht[-1])+np.dot(wxh[2],xt)+b[2])
            gt.append(np.dot(whh[3],ht[-1])+np.dot(wxh[3],xt)+b[3])
            ct.append(ft[-1] * ct[-1] + it[-1] * gt[-1])  #f(t)*c(t-1)+i(t)*g(t)
            ht.append(ot[-1] * np.tanh(ct[-1]))  #o(t)*tanh(c(t))
            output.append(np.dot(why, ht[-1]))
        return [[ft, it, ot, gt], np.array(ct), np.array(ht), np.array(output[-num:]).reshape(num,self.n_cells)]

    def forpass_lstm_full(self, x, y):
        # embedding layer -  x is already embedded in dataloader
        # hidden layer1
        fiog1,c1_out, h1_out, z1_out  = self.forpass_lstm(x, self.wxh1, self.whh1, self.why1, self.b1, self.s_len)
        # dropout1
        dp1_out = self.dropout(z1_out, self.dp)
        # hidden layer2
        fiog2,c2_out, h2_out, z2_out = self.forpass_lstm(dp1_out,self.wxh2,self.whh2,self.why2, self.b2,1)
        # dropout 2
        dp2_out = self.dropout(z2_out, self.dp)
        #fully connected
        fc_out = self.softmax(np.dot(self.w3, dp2_out.T)*100+self.b3)
        # cross entropy loss
        loss = -np.log(fc_out[y])
        return [x, fiog1, c1_out, h1_out, z1_out, 
                fiog2, c2_out, h2_out, z2_out, fc_out, loss, y]

    # backpropagation
    def backprop_lstm(self, x ,wxh, whh, why, b, dyt, fiog, ct, ht):
        ft, it, ot, gt = fiog
        dft, dit, dot, dgt = [],[],[],[]
        dwxh, dwhh, db = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(b)
        dwhy = np.zeros_like(why)
        dct = []
        dh = []
        din = []
        out_num = dyt.shape[0] #10 or 1
        for idx in range(len(x)):   # go reverse in input layer
            if out_num >= idx+1:
                dytt = dyt[-idx-1].reshape(self.n_cells,1)
                dwhy += (np.dot(dytt,ht[-idx-1].T)) # dwhy = ht*dyt
                dh.append(np.dot(why.T, dytt))   # dht = dht-why*dyt
            dct.append(dh[idx] * (1-(np.tanh(ct[-idx-1]))**2) * ot[-idx-1])   # dct = (1-tanh^2(ct))*ot*dht
            dot.append(dh[idx] * np.tanh(ct[-idx-1]))  # dht * tanh(ct)
            dft.append(dct[idx] * ct[-idx-2])   #dft = dct*c(t-1)
            dit.append(dct[idx] * gt[-idx-1])   #dit = dct*gt
            dgt.append(dct[idx] * it[-idx-1])   #dgt = dct*it
            
            ddot = (1-ot[-idx-1])*ot[-idx-1]*dot[idx]
            ddft = (1-ft[-idx-1])*ft[-idx-1]*dft[idx]
            ddit = (1-it[-idx-1])*it[-idx-1]*dit[idx]
            ddgt = (1-gt[-idx-1]**2)*dgt[idx]

            # bias gradient
            db[0] += sum(ddft)/self.n_cells
            db[1] += sum(ddit)/self.n_cells
            db[2] += sum(ddot)/self.n_cells
            db[3] += sum(ddgt)/self.n_cells

            # hidden-hidden weight gradient
            dwhh[0] += (np.dot(ddft, ht[-idx-2].T))  #dwhh = dd_all*h(t-1)
            dwhh[1] += (np.dot(ddit, ht[-idx-2].T)) 
            dwhh[2] += (np.dot(ddot, ht[-idx-2].T))
            dwhh[3] += (np.dot(ddgt, ht[-idx-2].T))

            # hidden layer gradient
            if out_num <= idx+1:
                dh_temp = np.dot(whh[0].T, ddft)+np.dot(whh[1].T, ddit)+np.dot(whh[2].T, ddot)+np.dot(whh[3].T, ddgt)
                dh.append(dh_temp/4.0)

            # input-hidden weight gradient
            xt = x[-idx-1].reshape(x.shape[1],1)  # from 0 dim to 1 dim
            dwxh[0] += (np.dot(ddft, xt.T))
            dwxh[1] += (np.dot(ddit, xt.T))
            dwxh[2] += (np.dot(ddot, xt.T))
            dwxh[3] += (np.dot(ddgt, xt.T))

            # input layer gradient
            din_temp = np.dot(wxh[0].T, ddft)+np.dot(wxh[1].T, ddit)+np.dot(wxh[2].T, ddot)+np.dot(wxh[3].T, ddgt)
            din.append(din_temp/4.0)
        return dwxh,dwhh,dwhy, db, np.array(din)[::-1]

    def backprop_lstm_full(self, forpass_log):
        # [x, fiog1, c1_out, h1_out, dp1_out, fiog2, c2_out, h2_out, dp2_out, fc_out, loss, y]
        x, fiog1, c1_out, h1_out, dp1_out, fiog2, c2_out, h2_out, dp2_out, fc_out, loss, y = forpass_log
        # fully connected layer backprop(softmax dev)
        dev3 = fc_out
        dev3[y] -= 1
        w3_grad= np.dot(dev3.reshape(self.label_size, 1), dp2_out.reshape(self.n_cells,1).T)  # linear layer deviation(for weight)
        dev2 = np.dot(self.w3.T, dev3)
        b3_grad= np.mean(dev3)

        # hidden reccurent layer backprop
        # self, x ,wxh, whh, why, b, dyt, fiog, ct, ht
        wxh2_grad,whh2_grad,why2_grad,b2_grad,dev1 = self.backprop_lstm(dp1_out,self.wxh2, self.whh2, self.why2, self.b2, dev2.T, fiog2, c2_out, h2_out)
        wxh1_grad,whh1_grad,why1_grad,b1_grad,dev0 = self.backprop_lstm(x, self.wxh1, self.whh1, self.why1, self.b1, dev1, fiog1, c1_out, h1_out)

        return wxh1_grad, whh1_grad, why1_grad, wxh2_grad, whh2_grad, why2_grad, w3_grad, b1_grad, b2_grad, b3_grad

    #==============================TRAINING! SGD====================================
    def train_model_sgd(self, trainLoader, testLoader, epochs):
        # initialize weights
        if self.cell_type == 0: # do RNN
            self.initialize_rnn()
        else:   # do LSTM
            self.initialize_lstm()
        # training start!
        for i in range(epochs):
            cnt=0
            train_loss = 0
            train_acc = 0
            self.s_len = 10
            for iter in range(len(trainLoader)):     # per every iteration
                wxh1_grad, whh1_grad, why1_grad, wxh2_grad, whh2_grad, why2_grad, w3_grad, b1_grad, b2_grad, b3_grad = (0,0,0,0,0,0,0,0,0,0)
                x_batch, y_batch = trainLoader[iter]
                batch_len = x_batch.shape[0]
                for j in range(batch_len):
                    cnt+=1
                    x_inst = x_batch[j]
                    y_inst = y_batch[j]
                    wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = (0,0,0,0,0,0,0,0,0,0)
                    if self.cell_type == 0: # do RNN
                        log_lst = self.forpass_rnn_full(x_inst, y_inst) # forpass -> get y_hat
                        # log accuracy and loss
                        if y_inst == log_lst[5].argmax():
                            train_acc += 1
                        train_loss += log_lst[6][0]
                        # get gradient
                        wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = self.backprop_rnn_full(log_lst)    # backpropagation -> update gradient

                    else:   # do LSTM
                        #[x, fiog1, c1_out, h1_out, dp1_out, fiog2, c2_out, h2_out, dp2_out, fc_out, loss, y]
                        log_lst = self.forpass_lstm_full(x_inst, y_inst)
                        # log accuracy and loss
                        if y_inst == log_lst[9].argmax():
                            train_acc += 1
                        train_loss += log_lst[10][0]
                        # get gradient
                        wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = self.backprop_lstm_full(log_lst)
                   
                    wxh1_grad += wxh1
                    whh1_grad += whh1
                    why1_grad += why1
                    wxh2_grad += wxh2
                    whh2_grad += whh2
                    why2_grad += why2
                    w3_grad += w3
                    b1_grad += b1
                    b2_grad += b2
                    b3_grad += b3

                # update average weight & bias gradient every batch.
                self.wxh1 -= wxh1_grad/batch_len * self.lr
                self.whh1 -= whh1_grad/batch_len * self.lr
                self.why1 -= why1_grad/batch_len * self.lr
                self.wxh2 -= wxh2_grad/batch_len * self.lr
                self.whh2 -= whh2_grad/batch_len * self.lr
                self.why2 -= why2_grad/batch_len * self.lr
                self.w3 -= w3_grad/batch_len * self.lr
                self.b1 -= b1_grad/batch_len * self.lr
                self.b2 -= b2_grad/batch_len * self.lr
                self.b3 -= b3_grad/batch_len * self.lr
                #print(wxh1_grad.shape, whh1_grad.shape, why1_grad.shape, wxh2_grad.shape, whh2_grad.shape, why2_grad.shape, w3_grad.shape, b1_grad.shape, b2_grad.shape, b3_grad.shape)

                if((iter+1) % (len(trainLoader))  == 0):
                    self.s_len=8
                    test_loss = 0
                    test_acc = 0
                    test_len = len(testLoader)
                    for j in range(test_len):
                        x_test_inst, y_test_inst = testLoader[j]
                        x_test_inst=x_test_inst[0]
                        y_test_inst=y_test_inst[0]
                        log_lst_test = []
                        if self.cell_type == 0: # do RNN
                        # forpass -> get y_hat
                            log_lst_test = self.forpass_rnn_full(x_test_inst, y_test_inst)
                            test_loss += log_lst_test[6][0] # get loss
                            if y_test_inst == log_lst_test[5].argmax(): #get accuracy
                                test_acc += 1
                        else:   # do LSTM
                            log_lst_test = self.forpass_lstm_full(x_test_inst, y_test_inst)
                            test_loss += log_lst_test[10][0] # get loss
                            if y_test_inst == log_lst_test[9].argmax(): #get accuracy
                                test_acc += 1
                    train_loss_tot = train_loss / (batch_len*(len(trainLoader)))
                    train_acc_tot = train_acc / (batch_len*(len(trainLoader)))
                    test_loss_tot = test_loss / test_len
                    test_acc_tot = test_acc / test_len
                    self.loss_log.append(train_loss_tot)
                    self.test_loss_log.append(test_loss_tot)
                    self.acc_log.append(train_acc_tot)
                    self.test_acc_log.append(test_acc_tot)
                    """
                    print("epoch: {0}".format(i) , ", sentences: ", cnt, ", train loss: {0:0.4f}".format(train_loss_tot), ", test loss: {0:0.4f}".format(test_loss_tot), 
                                    ", train accuracy: {0:0.2f}".format(train_acc_tot), ", test accuracy: {0:0.2f}".format(test_acc_tot))
                    """
                    if(i % 50 ==0 or i == epochs-1):
                        print("epoch: {0}".format(i) , ", sentences: ", cnt, ", train loss: {0:0.4f}".format(train_loss_tot), ", test loss: {0:0.4f}".format(test_loss_tot), 
                                    ", train accuracy: {0:0.2f}".format(train_acc_tot), ", test accuracy: {0:0.2f}".format(test_acc_tot))
                    train_loss = 0

    #=====================TRAINING! ADAM===============================
    def train_model_adam(self, trainLoader, testLoader, epochs = 10):
        # initialize weights
        if self.cell_type == 0: # do RNN
            self.initialize_rnn()
        else:   # do LSTM
            self.initialize_lstm()

        # initialize adam parameters
        beta1 = 0.9
        beta2 = 0.999
        epsilon = float(10e-8)
        m = {}
        m_hat = {}
        v = {}
        v_hat = {}
        params = {"wxh1":self.wxh1, "whh1":self.whh1, "why1":self.why1, "wxh2":self.wxh2, "whh2":self.whh2, "why2":self.why2, "w3":self.w3, 
                  "b1":self.b1, "b2":self.b2, "b3":np.array(self.b3)}
        for p in params:
            m[p] = np.zeros(params[p].shape)
            m_hat[p] = np.zeros(params[p].shape)
            v[p] = np.zeros(params[p].shape)
            v_hat[p] = np.zeros(params[p].shape)
        
        # training start!
        for i in range(epochs):
            cnt=0
            train_loss = 0
            train_acc = 0
            self.s_len = 10
            for iter in range(len(trainLoader)):     # per every iteration
                wxh1_grad, whh1_grad, why1_grad, wxh2_grad, whh2_grad, why2_grad, w3_grad, b1_grad, b2_grad, b3_grad = (0,0,0,0,0,0,0,0,0,0)
                x_batch, y_batch = trainLoader[iter]
                batch_len = x_batch.shape[0]
                for j in range(batch_len):
                    cnt+=1
                    x_inst = x_batch[j]
                    y_inst = y_batch[j]
                    wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = (0,0,0,0,0,0,0,0,0,0)
                    if self.cell_type == 0: # do RNN
                        log_lst = self.forpass_rnn_full(x_inst, y_inst) # forpass -> get y_hat
                        # log accuracy and loss
                        if y_inst == log_lst[5].argmax():
                            train_acc += 1
                        train_loss += log_lst[6][0]
                        # get gradient
                        wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = self.backprop_rnn_full(log_lst)    # backpropagation -> update gradient

                    else:   # do LSTM
                        #[x, fiog1, c1_out, h1_out, dp1_out, fiog2, c2_out, h2_out, dp2_out, fc_out, loss, y]
                        log_lst = self.forpass_lstm_full(x_inst, y_inst)
                        # log accuracy and loss
                        if y_inst == log_lst[9].argmax():
                            train_acc += 1
                        train_loss += log_lst[10][0]
                        # get gradient
                        wxh1, whh1, why1, wxh2, whh2, why2, w3, b1, b2, b3 = self.backprop_lstm_full(log_lst)
                   
                    wxh1_grad += wxh1
                    whh1_grad += whh1
                    why1_grad += why1
                    wxh2_grad += wxh2
                    whh2_grad += whh2
                    why2_grad += why2
                    w3_grad += w3
                    b1_grad += b1
                    b2_grad += b2
                    b3_grad += b3

                # update average weight & bias gradient every batch.
                grads = {"wxh1":wxh1_grad/batch_len, "whh1":whh1_grad/batch_len, "why1":why1_grad/batch_len, 
                         "wxh2":wxh2_grad/batch_len, "whh2":whh2_grad/batch_len, "why2":why2_grad/batch_len, 
                         "w3":w3_grad/batch_len, "b1":b1_grad/batch_len, "b2":b2_grad/batch_len, "b3":b3_grad/batch_len}
        
                for p in params:
                    m[p] = beta1 * m[p] + (1-beta1)*grads[p]
                    m_hat[p] = m[p] / (1-beta1*beta1)
                    v[p] = beta2 * v[p] + (1-beta2)*(grads[p]*grads[p])
                    v_hat[p] = v[p] / (1-beta2*beta2)
                    params[p] -= self.lr*m_hat[p]/np.sqrt(v_hat[p] + epsilon)

                if((iter+1) % (len(trainLoader))  == 0):
                    self.s_len=8
                    test_loss = 0
                    test_acc = 0
                    test_len = len(testLoader)
                    for j in range(test_len):
                        x_test_inst, y_test_inst = testLoader[j]
                        x_test_inst=x_test_inst[0]
                        y_test_inst=y_test_inst[0]
                        log_lst_test = []
                        if self.cell_type == 0: # do RNN
                        # forpass -> get y_hat
                            log_lst_test = self.forpass_rnn_full(x_test_inst, y_test_inst)
                            test_loss += log_lst_test[6][0] # get loss
                            if y_test_inst == log_lst_test[5].argmax(): #get accuracy
                                test_acc += 1
                        else:   # do LSTM
                            log_lst_test = self.forpass_lstm_full(x_test_inst, y_test_inst)
                            test_loss += log_lst_test[10][0] # get loss
                            if y_test_inst == log_lst_test[9].argmax(): #get accuracy
                                test_acc += 1

                    train_loss_tot = train_loss / (batch_len*(len(trainLoader)))
                    train_acc_tot = train_acc / (batch_len*(len(trainLoader)))
                    test_loss_tot = test_loss / test_len
                    test_acc_tot = test_acc / test_len
                    self.loss_log.append(train_loss_tot)
                    self.test_loss_log.append(test_loss_tot)
                    self.acc_log.append(train_acc_tot)
                    self.test_acc_log.append(test_acc_tot)

                    if(i % 50 ==0 or i == epochs-1):
                        print("epoch: {0}".format(i) , ", sentences: ", cnt, ", train loss: {0:0.4f}".format(train_loss_tot), ", test loss: {0:0.4f}".format(test_loss_tot), 
                                    ", train accuracy: {0:0.2f}".format(train_acc_tot), ", test accuracy: {0:0.2f}".format(test_acc_tot))
                    train_loss = 0

    #=======================TRAIN and predict utils=========================
    def train(self, trainLoader, testLoader, epochs = 10):
        if self.optimizer == 0:   # optimizer SGD
            self.train_model_sgd(trainLoader, testLoader, epochs)
        elif self.optimizer == 1: # optimizer ADAM
            self.train_model_adam(trainLoader, testLoader, epochs)

    def predict(self, testLoader):
        pred = []
        test_len = len(testLoader)
        for j in range(test_len):
            x_test_inst, y_test_inst = testLoader[j]
            x_test_inst=x_test_inst[0]
            log_lst_test = []
            if self.cell_type == 0: # do RNN
                log_lst_test = self.forpass_rnn_full(x_test_inst, y_test_inst)
                pred.append(log_lst_test[5].argmax())
            else:   # do LSTM
                log_lst_test = self.forpass_lstm_full(x_test_inst, y_test_inst)
                pred.append(log_lst_test[9].argmax())
        return pred

#%%
"""#Test and Analysis"""

# print emojis for test data
emoji_dict = {"0": "\u2764\uFE0F",    # :heart: prints a black instead of red heart depending on the font
                    "1": ":baseball:",
                    "2": ":smile:",
                    "3": ":disappointed:",
                    "4": ":fork_and_knife:"}

def print_predictions(X, pred):
    print()
    for i in range(X.shape[0]):
        print(emoji.emojize(emoji_dict[str(int(pred[i]))], use_aliases=True),'  ',X[i])

def result(model, version, glove=50):
    # plot loss grah
    if glove == 50:
        testdata = testLoad
    else:
        testdata = testLoad_100
    plt.plot(model.loss_log)
    plt.plot(model.test_loss_log)
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend(['train_loss', 'test_loss'])
    plt.title("Loss graph, "+version)
    plt.show()

    plt.plot(model.acc_log)
    plt.plot(model.test_acc_log)
    plt.xlabel('epoch')
    plt.ylabel('acc')
    plt.legend(['train_acc', 'test_acc'])
    plt.title("Accuracy graph, "+version)
    plt.show()

    pred = model.predict(testdata)
    emoj_test = pd.read_csv('/content/drive/MyDrive/test_emoji.csv',names = ['sentence', 'label'], header=None, usecols = [1,0])
    X = np.array(emoj_test['sentence'])
    print_predictions(X, pred)

# RNN, SGD, 50d
rna = RecurrentNetwork(128, 50, 0, 0, 0.005)  # n_cells, feature_size, SGD/ADAM, RNN/LSTM, lr
rna.train(trainLoad, testLoad, epochs=150)
result(rna, "RNN+SGD+50")

# lstm, SGD, 50d
rnb = RecurrentNetwork(128, 50, 0, 1, 0.005)  # n_cells, feature_size, SGD/ADAM, RNN/LSTM, lr
rnb.train(trainLoad, testLoad, epochs=150)

result(rnb, "LSTM+SGD+50")

# LSTM+ADAM+50d
rnc = RecurrentNetwork(128, 50, 1, 1, 0.001)  # n_cells, feature_size, SGD/ADAM, RNN/LSTM, lr
rnc.train(trainLoad, testLoad, epochs=150)
result(rnc, "LSTM+ADAM+50d")

# LSTM+SGD+100d
rnd = RecurrentNetwork(128, 100, 0, 1, 0.005)  # n_cells, feature_size, SGD/ADAM, RNN/LSTM, lr
rnd.train(trainLoad_100, testLoad_100, epochs=150)
result(rnd, "LSTM+SGD+100", 100)

# LSTM+SGD+50d+dropout
rne = RecurrentNetwork(128, 50, 0, 1, 0.005, dropout = 0.1)  # n_cells, feature_size, SGD/ADAM, RNN/LSTM, lr
rne.train(trainLoad, testLoad, epochs=150)
result(rne, "LSTM+SGD+50d+dropout")